# =============================================================================
# LLM Provider Configuration
# =============================================================================
# Choose between "ollama" or "zai"
LLM_PROVIDER=ollama

# LLM Model
# For Ollama: mistral, llama2, codellama, gpt-oss-20b, llama3.1:8b, etc.
# For Z.AI: glm-4.6, etc.
LLM_MODEL=llama3.1:8b

# Temperature (0.0 - 1.0)
# Lower = more deterministic, Higher = more creative
LLM_TEMPERATURE=0.2

# =============================================================================
# Ollama Configuration (if LLM_PROVIDER=ollama)
# =============================================================================

# Ollama base URL
# Local development: http://localhost:11434
# Docker Compose: http://ollama:11434
# Remote server: http://your-ollama-server:11434
OLLAMA_URL=http://localhost:11434

# =============================================================================
# Z.AI Configuration (if LLM_PROVIDER=zai)
# =============================================================================

# Z.AI API Key (required for Z.AI)
# Get your API key from https://z.ai
ZAI_API_KEY=

# Z.AI Base URL
# Options:
# - https://api.z.ai/api/coding/paas/v4 (coding endpoint)
# - https://api.z.ai/api/paas/v4 (general endpoint)
ZAI_BASE_URL=https://api.z.ai/api/coding/paas/v4

# =============================================================================
# Application Configuration
# =============================================================================

# CORS allowed origins (comma-separated for frontend access)
CORS_ORIGINS=http://localhost:3000,http://localhost:5173

# Max orchestration steps per request
MAX_ORCHESTRATION_STEPS=10

# =============================================================================
# Example Configurations
# =============================================================================

# Example 1: Using Ollama with Mistral
# LLM_PROVIDER=ollama
# LLM_MODEL=mistral
# OLLAMA_URL=http://localhost:11434

# Example 2: Using Z.AI with GLM-4.6
# LLM_PROVIDER=zai
# LLM_MODEL=glm-4.6
# ZAI_API_KEY=your-api-key-here
# ZAI_BASE_URL=https://api.z.ai/api/coding/paas/v4

# Example 3: Using Z.AI with Thai language support
# LLM_PROVIDER=zai
# LLM_MODEL=glm-4.6
# ZAI_API_KEY=your-api-key-here
# LLM_TEMPERATURE=0.7
