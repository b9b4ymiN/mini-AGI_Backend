# LLM Provider Configuration
# Choose between "ollama" or "zai"
LLM_PROVIDER=ollama

# LLM Model
# For Ollama: mistral, llama2, codellama, gpt-oss-20b, llama3.1:8b, etc.
# For Z.AI: glm-4.6, etc.
LLM_MODEL=llama3.1:8b

# Temperature (0.0 - 1.0)
# Lower = more deterministic, Higher = more creative
LLM_TEMPERATURE=0.2

# ============================================================================
# Ollama Configuration (if LLM_PROVIDER=ollama)
# ============================================================================

# Ollama base URL
OLLAMA_URL=http://localhost:11434

# ============================================================================
# Z.AI Configuration (if LLM_PROVIDER=zai)
# ============================================================================

# Z.AI API Key (required for Z.AI)
# Get your API key from https://z.ai
ZAI_API_KEY=

# Z.AI Base URL
# Options:
# - https://api.z.ai/api/coding/paas/v4 (coding endpoint)
# - https://api.z.ai/api/paas/v4 (general endpoint)
ZAI_BASE_URL=https://api.z.ai/api/coding/paas/v4

# ============================================================================
# Example Configurations
# ============================================================================

# Example 1: Using Ollama with Mistral
# LLM_PROVIDER=ollama
# LLM_MODEL=mistral
# OLLAMA_URL=http://localhost:11434

# Example 2: Using Z.AI with GLM-4.6
# LLM_PROVIDER=zai
# LLM_MODEL=glm-4.6
# ZAI_API_KEY=your-api-key-here
# ZAI_BASE_URL=https://api.z.ai/api/coding/paas/v4

# Example 3: Using Z.AI with Thai language support
# LLM_PROVIDER=zai
# LLM_MODEL=glm-4.6
# ZAI_API_KEY=your-api-key-here
# LLM_TEMPERATURE=0.7
