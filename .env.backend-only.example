# =============================================================================
# Backend-Only Configuration (No Ollama Container)
# =============================================================================
# Use this when you DON'T need the Ollama container because:
# - You're using Z.AI cloud API
# - You're using an external Ollama server
# - Ollama is already running on your host machine
#
# Copy: cp .env.backend-only.example .env
# =============================================================================

# =============================================================================
# Option 1: Using Z.AI Cloud API (Recommended)
# =============================================================================
LLM_PROVIDER=zai
LLM_MODEL=glm-4.6
LLM_TEMPERATURE=0.2
ZAI_API_KEY=your-zai-api-key-here
ZAI_BASE_URL=https://api.z.ai/api/coding/paas/v4

# =============================================================================
# Option 2: Using External Ollama Server
# =============================================================================
# LLM_PROVIDER=ollama
# LLM_MODEL=llama3.1:8b
# LLM_TEMPERATURE=0.2
# OLLAMA_URL=http://your-ollama-server:11434

# =============================================================================
# Option 3: Using Ollama on Host Machine (Windows/Mac)
# =============================================================================
# LLM_PROVIDER=ollama
# LLM_MODEL=llama3.1:8b
# LLM_TEMPERATURE=0.2
# OLLAMA_URL=http://host.docker.internal:11434

# =============================================================================
# Application Configuration
# =============================================================================
CORS_ORIGINS=http://localhost:3000,http://localhost:5173
MAX_ORCHESTRATION_STEPS=10
BACKEND_PORT=8000
